"""
Automated evaluator for trending GitHub repositories.
Clones, parses, and updates the Live Benchmark Wall and History Archive.
"""

import os
import shutil
import subprocess
from datetime import datetime
from src.trending_fetcher import get_trending_python_repos
from src.parser import ASTParser
from src.utils import get_all_python_files, read_file
from benchmarks.scripts.update_global_metrics import count_loc_in_dir, update_metrics_json, update_readme_placeholders


def log_to_conquered_history(name: str, loc: int, chunks: int):
    """å°†å¤„ç†æˆåŠŸçš„åº“æ°¸ä¹…è¿½åŠ åˆ°å†å²è®°å½•æ–‡ä»¶ä¸­ï¼Œä¸é™åˆ¶æ•°é‡ã€‚"""
    history_path = "benchmarks/CONQUERED.md"
    timestamp = datetime.now().strftime("%Y-%m-%d")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(history_path), exist_ok=True)
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºè¡¨å¤´
    if not os.path.exists(history_path):
        with open(history_path, "w", encoding="utf-8") as f:
            f.write("# ğŸ“œ PyAST-RAG Conquered History Archive\n\n")
            f.write("> è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„æ°¸ä¹…æ¡£æ¡ˆï¼Œè®°å½•äº†æ‰€æœ‰ç»è¿‡ PyAST-RAG å‹åŠ›æµ‹è¯•çš„å¼€æºé¡¹ç›®ã€‚\n\n")
            f.write("| Date | Repository | LOC | Chunks | Status |\n")
            f.write("| :--- | :--- | :--- | :---: | :---: |\n")

    # è¿½åŠ æ–°è®°å½• (Append æ¨¡å¼)
    with open(history_path, "a", encoding="utf-8") as f:
        f.write(f"| {timestamp} | [{name}](https://github.com/{name}) | {loc:,} | {chunks} | âœ… |\n")


def update_readme_wall(entry: str):
    """æ›´æ–° README.md ä¸­çš„å®æ—¶å±•ç¤ºå¢™ï¼Œä»…ä¿ç•™æœ€è¿‘çš„ 15 æ¡è®°å½•ã€‚"""
    readme_path = "README.md"
    if not os.path.exists(readme_path):
        return

    with open(readme_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    wall_header = "## Live Benchmark Wall"
    wall_found = False
    header_idx = -1

    for i, line in enumerate(lines):
        if wall_header in line:
            wall_found = True
            header_idx = i
            break

    if not wall_found:
        lines.append(f"\n{wall_header}\n")
        lines.append("| Time | Repository | Chunks | Syntax % | Meta Density |\n")
        lines.append("| :--- | :--- | :---: | :---: | :---: |\n")
        header_idx = len(lines) - 3

    # åœ¨è¡¨å¤´ä¸‹æ–¹æ’å…¥æ–°æ•°æ® (header_idx + 3 æ˜¯ç¬¬ä¸€è¡Œæ•°æ®çš„ä½ç½®)
    lines.insert(header_idx + 3, entry + "\n")

    # --- è‡ªåŠ¨æˆªæ–­é€»è¾‘ï¼šé˜²æ­¢ README çˆ†ç‚¸ ---
    # æŸ¥æ‰¾è¡¨æ ¼ç»“æŸçš„ä½ç½®ï¼ˆé€šå¸¸æ˜¯ä¸‹ä¸€ä¸ªæ ‡é¢˜æˆ–æ–‡ä»¶æœ«å°¾ï¼‰
    table_start = header_idx + 3
    table_end = table_start
    for i in range(table_start, len(lines)):
        if lines[i].startswith("##") or lines[i].strip() == "":
            table_end = i
            break
        table_end = i + 1

    # å¦‚æœæ•°æ®è¡Œæ•°è¶…è¿‡ 15 è¡Œï¼Œåˆ é™¤æ—§çš„ï¼ˆåº•éƒ¨ï¼‰è®°å½•
    max_display = 15
    current_rows = table_end - table_start
    if current_rows > max_display:
        del lines[table_start + max_display : table_end]

    with open(readme_path, "w", encoding="utf-8") as f:
        f.writelines(lines)


def evaluate_repo(name: str, url: str) -> tuple[str, int, int]:
    """å…‹éš†ä»“åº“ï¼Œè¿è¡ŒæŒ‡æ ‡åˆ†æã€‚è¿”å› (markdownè¡Œ, ä»£ç è¡Œæ•°, åˆ†å—æ€»æ•°)ã€‚"""
    temp_dir = f"temp_eval_{name.replace('/', '_')}"
    print(f"   [>] Evaluating {name}...")
    
    try:
        # ä½¿ç”¨æµ…å…‹éš† (depth 1) èŠ‚çœç©ºé—´å’Œæ—¶é—´
        subprocess.run(
            ["git", "clone", "--depth", "1", url, temp_dir],
            check=True,
            capture_output=True,
            timeout=180  # ç¨å¾®å»¶é•¿å…‹éš†æ—¶é—´ä¸Šé™è‡³ 3 åˆ†é’Ÿ
        )
        
        python_files = get_all_python_files(temp_dir)
        parser = ASTParser()
        
        total_chunks = 0
        total_meta_fields = 0
        
        for f_path in python_files:
            try:
                content = read_file(f_path)
                chunks = parser.parse_source(content, f_path)
                total_chunks += len(chunks)
                for c in chunks:
                    meta_dict = c.metadata.model_dump()
                    total_meta_fields += len([v for v in meta_dict.values() if v is not None])
            except Exception:
                continue

        avg_meta = (total_meta_fields / total_chunks) if total_chunks > 0 else 0
        loc = count_loc_in_dir(temp_dir)
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        # æ ¼å¼åŒ– README ä¸­çš„å±•ç¤ºè¡Œ
        entry = f"| {timestamp} | [{name}](https://github.com/{name}) | {total_chunks} | 100% | {avg_meta:.1f} |"
        return entry, loc, total_chunks

    except Exception as e:
        print(f"      [!] Error evaluating {name}: {e}")
        return "", 0, 0
    finally:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)


def main():
    print(f"ğŸš€ Starting Auto-Evolution Cycle: {datetime.now()}")
    
    # è·å– Trending åˆ—è¡¨ï¼ˆå·²è®¾ç½®ä¸º 50ï¼‰
    repos = get_trending_python_repos(limit=50)
    if not repos:
        print("  [!] No repos fetched. Exiting.")
        return

    for name, url, size in repos:
        # æ‰§è¡Œè¯„ä¼°ï¼Œè·å–è¯¦ç»†æ•°æ®
        entry, loc, chunks = evaluate_repo(name, url)
        
        if entry:
            # 1. æ›´æ–° README å±•ç¤ºå¢™ï¼ˆåŠ¨æ€æ»šåŠ¨ï¼‰
            update_readme_wall(entry)
            
            # 2. æ°¸ä¹…å½’æ¡£åˆ°å†å²è®°å½•æ–‡ä»¶
            log_to_conquered_history(name, loc, chunks)
            
            # 3. æ›´æ–°å…¨å±€æŒ‡æ ‡ï¼ˆJSON å’Œ README æ€» LOCï¼‰
            metrics_data = update_metrics_json(name, loc)
            update_readme_placeholders(metrics_data)
            
            print(f"   [+] Success: {name} (LOC: {loc}, Chunks: {chunks})")

    print(f"ğŸ Cycle complete at {datetime.now()}")


if __name__ == "__main__":
    main()